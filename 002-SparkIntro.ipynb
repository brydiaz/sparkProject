{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYQE37rOc539"
   },
   "source": [
    "# ITCR - Sede Interuniversitaria de Alajuela\n",
    "\n",
    "## Introducción a Spark\n",
    "\n",
    "### Profesora: María Mora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "### General\n",
    "\n",
    "Entender y aplicar técnicas de análisis de grandes cantidades de datos para la resolución de problemas concretos a través de tecnologías de manipulación, extracción y sintetización estadística. El ejemplo está basado en [1].\n",
    "\n",
    "Objetivos específicos\n",
    "\n",
    "- Aplicar bibliotecas para la transformación de datos a gran escala para poder sintetizar el conocimiento para futuros análisis.\n",
    "\n",
    "- Introduccir al estudiante en el uso de un framework para trabajo con big data como es Spark. \n",
    "\n",
    "\n",
    "# Datos\n",
    "\n",
    "Se utilizarán dos conjuntos de datos \n",
    "\n",
    "1) Tienda en línea. Datos generados automáticamente. A manera de ejemplo, se simulará la generación de datos para una tienda en línea. Se aplicarán ciertas operaciones básicas sobre los datos para mostrar lo que este tipo de plataformas ofrecen. \n",
    "\n",
    "2) New York Times Best Sellers. \n",
    "Conjunto de datos bajado de Kaggle, los datos recopilados incluyen el título del libro, el autor, la fecha de la lista de best sellers, la fecha de publicación de la lista, la descripción del libro, el editor, el número de semanas en la lista, el rango (esta semana y la semana pasada) y precio.\n",
    "\n",
    "Fuente: https://www.kaggle.com/cmenca/new-york-times-hardcover-fiction-best-sellers\n",
    "\n",
    "\n",
    "# Contenidos\n",
    "\n",
    "\n",
    "- Introducción al procesamiento a gran escala por medio de los siguientes ejemplos:\n",
    "- Ejemplo 1. Tienda en línea.  \n",
    "  - Ejemplo de procesamiento de data frames\n",
    "  - Procesamiento de atributos\n",
    "- Ejemplo 2. NYT Best Sellers.\n",
    "  - Procesamiento de data frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnWcYa4XLKsq"
   },
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vZaepF2QJrCt",
    "outputId": "6440398a-fda7-49a0-ec6f-0d2d20a6b44d"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas requeridas para el ejercicio\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession, Row, dataframe\n",
    "\n",
    "# generar valores enteros random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import random\n",
    "\n",
    "import findspark\n",
    "SPARK_PATH = '/opt/spark'\n",
    "findspark.init(SPARK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8chJbeF0dXmO"
   },
   "source": [
    "## 1. Introducción a procesamiento a gran escala\n",
    "\n",
    "\n",
    "Inicialización de la sesión de Spark, punto de entrada a la programación con el API de Datasets, DataFrame y funcionalidad SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"PysparkEj1\")\\\n",
    "#     .config (\"spark.sql.shuffle.partitions\", \"50\")\\\n",
    "#     .config(\"spark.driver.maxResultSize\",\"5g\")\\\n",
    "#     .config (\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "POSTGRESQL_URL = \"jdbc:postgresql://localhost/\"\n",
    "POSTGRESQL_USER = \"postgres\"\n",
    "POSTGRESQL_PASSWORD = \"big_data\"\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function builds a Spark Session\n",
    "    return the main entry of a Spark DataFrame\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "      .builder \\\n",
    "      .appName(\"Basic JDBC pipeline\") \\\n",
    "      .config(\"spark.driver.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "      .config(\"spark.executor.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "      .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "def get_random_date(year):\n",
    "    # Genera fechas random en el año solicitado por el usuario.\n",
    "    # Params: \n",
    "    #   year: año en el que se generará la fecha\n",
    "    \n",
    "    # try to get a date\n",
    "    try:\n",
    "        #retorna datetime.datetime.strptime('{} {}'.format(randint(1, 366), year), '%j %Y')\n",
    "        return datetime.strptime('{} {}'.format(randint(1, 366), year), '%j %Y')\n",
    "\n",
    "    # si el valor se encuentra en el rango de los años bisiestos, inténtelo de nuevo\n",
    "    except ValueError:\n",
    "        get_random_date(year)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 1. Tienda en línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id         purchased_at amount\n",
      "0           17  1972-02-02 00:00:00    292\n",
      "1           63  1997-08-19 00:00:00    498\n",
      "2           48  2000-04-17 00:00:00    139\n",
      "3            3  2014-07-19 00:00:00    461\n",
      "4           97  1998-01-02 00:00:00    711\n",
      "5           34  1992-04-27 00:00:00    611\n",
      "6           13  2015-06-12 00:00:00     79\n",
      "7            3  1983-10-05 00:00:00     58\n",
      "8           48  1987-04-21 00:00:00    970\n",
      "9           92  1903-09-28 00:00:00    260\n",
      "10          56  1963-10-11 00:00:00    271\n",
      "11          29  1986-04-23 00:00:00    772\n",
      "12          37  2018-01-12 00:00:00    445\n",
      "13          71  2018-11-25 00:00:00    145\n",
      "14          80  1992-05-31 00:00:00    164\n",
      "15          42  2014-12-31 00:00:00    525\n",
      "16          54  1964-12-09 00:00:00    230\n",
      "17          36  1975-09-13 00:00:00    853\n",
      "18          64  1950-10-29 00:00:00    860\n",
      "19          61  1931-07-26 00:00:00    443\n",
      "20          22  1946-10-08 00:00:00    888\n",
      "21          99  1986-07-11 00:00:00    132\n",
      "22          84  1965-02-25 00:00:00    789\n",
      "23          66  2007-07-21 00:00:00    401\n",
      "24          93  1903-08-29 00:00:00     91\n",
      "25          90  2008-11-10 00:00:00    613\n",
      "26          50  1982-03-29 00:00:00    210\n",
      "27          29  1901-04-13 00:00:00    562\n",
      "28          70  1929-07-27 00:00:00    538\n",
      "29          73  1945-08-24 00:00:00    914\n",
      "..         ...                  ...    ...\n",
      "70          27  1934-12-12 00:00:00    141\n",
      "71          48  2019-10-08 00:00:00    376\n",
      "72          87  1968-09-05 00:00:00    779\n",
      "73          68  1930-02-03 00:00:00    739\n",
      "74          10  1917-03-28 00:00:00    208\n",
      "75          68  1927-05-18 00:00:00    771\n",
      "76          76  1964-05-10 00:00:00    399\n",
      "77          43  1914-05-30 00:00:00    273\n",
      "78          77  1999-09-08 00:00:00    178\n",
      "79          70  1998-02-23 00:00:00    354\n",
      "80          52  1909-07-14 00:00:00    872\n",
      "81         100  1918-03-06 00:00:00    373\n",
      "82          78  1975-07-13 00:00:00    122\n",
      "83          70  1928-10-16 00:00:00    127\n",
      "84          34  1946-06-01 00:00:00    586\n",
      "85          14  1958-05-22 00:00:00    152\n",
      "86           5  2005-06-01 00:00:00     61\n",
      "87          85  1901-02-16 00:00:00    442\n",
      "88           5  1924-05-02 00:00:00    796\n",
      "89          75  1953-03-24 00:00:00    159\n",
      "90          21  1987-05-04 00:00:00    200\n",
      "91          13  1955-07-13 00:00:00    816\n",
      "92          69  2016-05-30 00:00:00    572\n",
      "93          91  1961-06-11 00:00:00    145\n",
      "94          83  1940-01-21 00:00:00     75\n",
      "95         100  2018-06-01 00:00:00    740\n",
      "96          40  1957-07-20 00:00:00    347\n",
      "97           8  1908-06-11 00:00:00    971\n",
      "98          58  1914-05-09 00:00:00    254\n",
      "99          79  1999-10-05 00:00:00    874\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generación de datos \n",
    "\n",
    "# Parametros\n",
    "\n",
    "maxCustomerId = 100     # Max id para clientes\n",
    "minAnio = 1900          # Min year para generar transacciones\n",
    "maxAnio = 2019          # Max year para generar transacciones\n",
    "maxValue = 1000         # Max value para amount\n",
    "minValue = 50           # Min value para amount\n",
    "amountTransactions = 100 # Numero de transacciones generadas automáticamente. \n",
    "\n",
    "def createTransactions(n) : \n",
    "   #Genera n transacciones. \n",
    "   #Parámetros: \n",
    "   #    n la cantidad de registros a ser generados.\n",
    "   \n",
    "   df = pd.DataFrame(columns=['customer_id', 'purchased_at', 'amount'])\n",
    "  \n",
    "   seed(1)\n",
    "   for x in range(n):\n",
    "      df.loc[x, 'customer_id'] = randint(0, maxCustomerId)\n",
    "      df.loc[x, 'purchased_at'] = get_random_date(randint(minAnio, maxAnio))\n",
    "      df.loc[x, 'amount'] = int(minValue + (random() * (maxValue - minValue)))\n",
    "       \n",
    "   return df\n",
    "\n",
    "#Crea un Pandas DataFrame\n",
    "dfPandas = createTransactions(amountTransactions)\n",
    "print(dfPandas)\n",
    "\n",
    "# Crea un Spark Data Frame\n",
    "df = spark.createDataFrame(dfPandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfromaciones básicas\n",
    "Spark provee funciones para realizar transformaciones a los datos. Por ejemplo, transformar fechas a una representación con formato específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------+-----------+\n",
      "|customer_id|       purchased_at|amount|date_string|\n",
      "+-----------+-------------------+------+-----------+\n",
      "|         17|1972-02-02 00:00:00|   292| 02/02/1972|\n",
      "|         63|1997-08-19 00:00:00|   498| 19/08/1997|\n",
      "|         48|2000-04-17 00:00:00|   139| 17/04/2000|\n",
      "|          3|2014-07-19 00:00:00|   461| 19/07/2014|\n",
      "|         97|1998-01-02 00:00:00|   711| 02/01/1998|\n",
      "|         34|1992-04-27 00:00:00|   611| 27/04/1992|\n",
      "|         13|2015-06-12 00:00:00|    79| 12/06/2015|\n",
      "|          3|1983-10-05 00:00:00|    58| 05/10/1983|\n",
      "|         48|1987-04-21 00:00:00|   970| 21/04/1987|\n",
      "|         92|1903-09-28 00:00:00|   260| 28/09/1903|\n",
      "|         56|1963-10-11 00:00:00|   271| 11/10/1963|\n",
      "|         29|1986-04-23 00:00:00|   772| 23/04/1986|\n",
      "|         37|2018-01-12 00:00:00|   445| 12/01/2018|\n",
      "|         71|2018-11-25 00:00:00|   145| 25/11/2018|\n",
      "|         80|1992-05-31 00:00:00|   164| 31/05/1992|\n",
      "|         42|2014-12-31 00:00:00|   525| 31/12/2014|\n",
      "|         54|1964-12-09 00:00:00|   230| 09/12/1964|\n",
      "|         36|1975-09-13 00:00:00|   853| 13/09/1975|\n",
      "|         64|1950-10-29 00:00:00|   860| 29/10/1950|\n",
      "|         61|1931-07-26 00:00:00|   443| 26/07/1931|\n",
      "+-----------+-------------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- purchased_at: timestamp (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nueva columna con los datos en el formato deseado\n",
    "formatted_df = df.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'dd/MM/yyyy'))\n",
    "formatted_df.show()\n",
    "\n",
    "# Estructura del dataframe\n",
    "formatted_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso que se necesite crear una función que no es parte de la biblioteca estándar en Spark, es posible definir funciones creadas por el usuario (User Defined Functions o udf). La noción básica de una udf en Spark es un lambda acompañado por el tipo de dato retornado. Lo anterior es estrictamente necesario en lenguajes con un sistema de tipos débil.\n",
    "\n",
    "Ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------+-----------+----------+\n",
      "|customer_id|       purchased_at|amount|date_string|      date|\n",
      "+-----------+-------------------+------+-----------+----------+\n",
      "|         17|1972-02-02 00:00:00|   292| 02/02/1972|1972-02-02|\n",
      "|         63|1997-08-19 00:00:00|   498| 19/08/1997|1997-08-19|\n",
      "|         48|2000-04-17 00:00:00|   139| 17/04/2000|2000-04-17|\n",
      "|          3|2014-07-19 00:00:00|   461| 19/07/2014|2014-07-19|\n",
      "|         97|1998-01-02 00:00:00|   711| 02/01/1998|1998-01-02|\n",
      "|         34|1992-04-27 00:00:00|   611| 27/04/1992|1992-04-27|\n",
      "|         13|2015-06-12 00:00:00|    79| 12/06/2015|2015-06-12|\n",
      "|          3|1983-10-05 00:00:00|    58| 05/10/1983|1983-10-05|\n",
      "|         48|1987-04-21 00:00:00|   970| 21/04/1987|1987-04-21|\n",
      "|         92|1903-09-28 00:00:00|   260| 28/09/1903|1903-09-28|\n",
      "|         56|1963-10-11 00:00:00|   271| 11/10/1963|1963-10-11|\n",
      "|         29|1986-04-23 00:00:00|   772| 23/04/1986|1986-04-23|\n",
      "|         37|2018-01-12 00:00:00|   445| 12/01/2018|2018-01-12|\n",
      "|         71|2018-11-25 00:00:00|   145| 25/11/2018|2018-11-25|\n",
      "|         80|1992-05-31 00:00:00|   164| 31/05/1992|1992-05-31|\n",
      "|         42|2014-12-31 00:00:00|   525| 31/12/2014|2014-12-31|\n",
      "|         54|1964-12-09 00:00:00|   230| 09/12/1964|1964-12-09|\n",
      "|         36|1975-09-13 00:00:00|   853| 13/09/1975|1975-09-13|\n",
      "|         64|1950-10-29 00:00:00|   860| 29/10/1950|1950-10-29|\n",
      "|         61|1931-07-26 00:00:00|   443| 26/07/1931|1931-07-26|\n",
      "+-----------+-------------------+------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- purchased_at: timestamp (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_to_date = \\\n",
    "    udf(lambda text_date: datetime.strptime(text_date, '%d/%m/%Y'),\n",
    "        DateType())\n",
    "\n",
    "typed_df = formatted_df.withColumn(\"date\", string_to_date(formatted_df.date_string))\n",
    "typed_df.show()\n",
    "typed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL. Para sumar los datos podemos utilizar agrupamiento igual que en SQL groupBy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------------+-----------+\n",
      "|customer_id|      date|sum(customer_id)|sum(amount)|\n",
      "+-----------+----------+----------------+-----------+\n",
      "|         75|1953-03-24|              75|        159|\n",
      "|         83|1940-01-21|              83|         75|\n",
      "|         57|1901-05-24|              57|        287|\n",
      "|         13|1955-07-13|              13|        816|\n",
      "|         17|1972-02-02|              17|        292|\n",
      "|         34|1992-04-27|              34|        611|\n",
      "|          2|1928-01-10|               2|        427|\n",
      "|         52|1909-07-14|              52|        872|\n",
      "|         67|1921-12-03|              67|        309|\n",
      "|         34|1946-06-01|              34|        586|\n",
      "|         86|1909-02-12|              86|        874|\n",
      "|         50|1982-03-29|              50|        210|\n",
      "|         71|2012-01-20|              71|        611|\n",
      "|         40|1957-07-20|              40|        347|\n",
      "|          4|1992-03-23|               4|        473|\n",
      "|         77|1999-09-08|              77|        178|\n",
      "|         29|1901-04-13|              29|        562|\n",
      "|         13|2015-06-12|              13|         79|\n",
      "|         14|2002-11-16|              14|        225|\n",
      "|         36|1975-09-13|              36|        853|\n",
      "+-----------+----------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sumar todas las compras por cliente y día:\n",
    "\n",
    "sum_df = typed_df.groupBy(\"customer_id\", \"date\").sum()\n",
    "sum_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark sumará todas las columnas que no se encuentren especificadas en la operación groupBy. Algunos resultados no tienen interpretación últil. Por ejemplo, sumar la columna customer_id no da ningún valor agregado. \n",
    "\n",
    "Finalmente, es posible dar a las columnas un nombre más apropiado para los usuarios de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|         75|1953-03-24|   159|\n",
      "|         83|1940-01-21|    75|\n",
      "|         57|1901-05-24|   287|\n",
      "|         13|1955-07-13|   816|\n",
      "|         17|1972-02-02|   292|\n",
      "|         34|1992-04-27|   611|\n",
      "|          2|1928-01-10|   427|\n",
      "|         52|1909-07-14|   872|\n",
      "|         67|1921-12-03|   309|\n",
      "|         34|1946-06-01|   586|\n",
      "|         86|1909-02-12|   874|\n",
      "|         50|1982-03-29|   210|\n",
      "|         71|2012-01-20|   611|\n",
      "|         40|1957-07-20|   347|\n",
      "|          4|1992-03-23|   473|\n",
      "|         77|1999-09-08|   178|\n",
      "|         29|1901-04-13|   562|\n",
      "|         13|2015-06-12|    79|\n",
      "|         14|2002-11-16|   225|\n",
      "|         36|1975-09-13|   853|\n",
      "+-----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats_df = \\\n",
    "    sum_df.select(\n",
    "        col('customer_id'),\n",
    "        col('date'),\n",
    "        col('sum(amount)').alias('amount'))\n",
    "\n",
    "stats_df.printSchema()\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permite cargar información de múltiples fuentes. A continuación se muestra como cargar datos de un archivo JSON que contiene datos del NYT Best Sellers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 2 NYT Best Sellers.\n",
    "\n",
    "A partir de un archivo json se creará un spark dataframe y se transformará los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea Dataframes de Spark a partir de  datos en formato JSON\n",
    "\n",
    "dataframe = spark.read.json('nyt2.json')\n",
    "\n",
    "#Archivos TXT# \n",
    "#dataframe = spark.read.text('text_data.txt')\n",
    "\n",
    "#Archivos CSV\n",
    "#dataframe_csv = spark.read.csv('csv_data.csv')\n",
    "\n",
    "#Archivos PARQUET\n",
    "# El formato Parquet es un formato open-source \n",
    "# utilizado para la serialización de datos (Apache Avro).\n",
    "# El formato admite una compresión y codificación muy \n",
    "# eficientes de datos orientados a columnas.\n",
    "#dataframe_parquet = spark.read.load('parquet_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de uso\n",
    "\n",
    "### 1. Exploración del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATOS GENERALES DEL DATAFRAME\n",
      "Cantidad de registros 10195 \n",
      "\n",
      "Desplegar las columnas DataFrame[summary: string, amazon_product_url: string, author: string, description: string, publisher: string, title: string] \n",
      "\n",
      "Desplegar los datos de algunas columnas\n",
      "+--------------------+--------------------+\n",
      "|              author|               title|\n",
      "+--------------------+--------------------+\n",
      "|       Dean R Koontz|           ODD HOURS|\n",
      "|     Stephenie Meyer|            THE HOST|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...|\n",
      "|   Patricia Cornwell|           THE FRONT|\n",
      "|     Chuck Palahniuk|               SNUFF|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANY’S|\n",
      "|       John Sandford|        PHANTOM PREY|\n",
      "|       Jimmy Buffett|          SWINE NOT?|\n",
      "|    Elizabeth George|     CARELESS IN RED|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Desplegar todos los datos\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|                 _id|  amazon_product_url|              author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|       Putnam| [4]|           [0]|           THE FRONT|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|    Doubleday| [5]|           [0]|               SNUFF|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|[[1211587200000]]|A woman finds an ...|[24.99,]|[[1212883200000]]|Little, Brown| [6]|           [3]|SUNDAYS AT TIFFANY’S|          [4]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       John Sandford|[[1211587200000]]|The Minneapolis d...|[26.95,]|[[1212883200000]]|       Putnam| [7]|           [4]|        PHANTOM PREY|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|[[1211587200000]]|A Southern family...|[21.99,]|[[1212883200000]]|Little, Brown| [8]|           [6]|          SWINE NOT?|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|[[1211587200000]]|In Cornwall, tryi...|[27.95,]|[[1212883200000]]|       Harper| [9]|           [8]|     CARELESS IN RED|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|[[1211587200000]]|An intelligence a...|[26.99,]|[[1212883200000]]|Grand Central|[10]|           [7]|     THE WHOLE TRUTH|          [5]|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"DATOS GENERALES DEL DATAFRAME\")\n",
    "print(\"Cantidad de registros\", dataframe.count(), \"\\n\")\n",
    "\n",
    "print(\"Desplegar las columnas\", dataframe.describe(), \"\\n\")\n",
    "\n",
    "print(\"Desplegar los datos de algunas columnas\")\n",
    "dataframe.select(\"author\", \"title\").show(10)\n",
    "\n",
    "print(\"Desplegar todos los datos\") \n",
    "dataframe.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remover duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros antes 20390 \n",
      "\n",
      "Cantidad de registros después de borrar duplicados 10195 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# El archivo no tiene registros duplicados por lo que primero \n",
    "# se carga dos veces los datos para generar duplicados\n",
    "dataframe2 = spark.read.json('nyt2.json')\n",
    "\n",
    "# Concatenar dos dataframes \n",
    "df_concat = dataframe.union(dataframe2)\n",
    "\n",
    "\n",
    "print(\"Cantidad de registros antes\", df_concat.count(), \"\\n\")\n",
    "\n",
    "dataframe_dropdup = df_concat.dropDuplicates() \n",
    "print(\"Cantidad de registros después de borrar duplicados\", dataframe_dropdup.count(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reemplazo when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------+\n",
      "|              author|               title|rank|   price|\n",
      "+--------------------+--------------------+----+--------+\n",
      "|       Dean R Koontz|           ODD HOURS| [1]|  [, 27]|\n",
      "|     Stephenie Meyer|            THE HOST| [2]|[25.99,]|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...| [3]|[24.95,]|\n",
      "|   Patricia Cornwell|           THE FRONT| [4]|[22.95,]|\n",
      "|     Chuck Palahniuk|               SNUFF| [5]|[24.95,]|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANY’S| [6]|[24.99,]|\n",
      "|       John Sandford|        PHANTOM PREY| [7]|[26.95,]|\n",
      "|       Jimmy Buffett|          SWINE NOT?| [8]|[21.99,]|\n",
      "|    Elizabeth George|     CARELESS IN RED| [9]|[27.95,]|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|[10]|[26.99,]|\n",
      "+--------------------+--------------------+----+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+------+\n",
      "|               title|result|\n",
      "+--------------------+------+\n",
      "|           ODD HOURS|     0|\n",
      "|            THE HOST|     1|\n",
      "|LOVE THE ONE YOU'...|     1|\n",
      "|           THE FRONT|     1|\n",
      "|               SNUFF|     1|\n",
      "|SUNDAYS AT TIFFANY’S|     1|\n",
      "|        PHANTOM PREY|     1|\n",
      "|          SWINE NOT?|     1|\n",
      "|     CARELESS IN RED|     1|\n",
      "|     THE WHOLE TRUTH|     1|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select(\"author\", \"title\", \"rank\", \"price\").show(10)\n",
    "\n",
    "dataframe.select(\"title\",when(dataframe.title != 'ODD HOURS',1).otherwise(0).alias(\"result\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condicionales: isin y is not in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------+\n",
      "|              author|               title|rank|   price|\n",
      "+--------------------+--------------------+----+--------+\n",
      "|     Stephenie Meyer|            THE HOST| [2]|[25.99,]|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...| [3]|[24.95,]|\n",
      "|   Patricia Cornwell|           THE FRONT| [4]|[22.95,]|\n",
      "|     Chuck Palahniuk|               SNUFF| [5]|[24.95,]|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANY’S| [6]|[24.99,]|\n",
      "|       Jimmy Buffett|          SWINE NOT?| [8]|[21.99,]|\n",
      "|    Elizabeth George|     CARELESS IN RED| [9]|[27.95,]|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|[10]|[26.99,]|\n",
      "|        Troy Denning|          INVINCIBLE|[11]|  [, 27]|\n",
      "|          James Frey|BRIGHT SHINY MORNING|[12]|[26.95,]|\n",
      "+--------------------+--------------------+----+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "+-------------+------------+----+--------+\n",
      "|       author|       title|rank|   price|\n",
      "+-------------+------------+----+--------+\n",
      "|Dean R Koontz|   ODD HOURS| [1]|  [, 27]|\n",
      "|John Sandford|PHANTOM PREY| [7]|[26.95,]|\n",
      "+-------------+------------+----+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# is not (~) in\n",
    "df_filtered = dataframe.filter(~dataframe[\"title\"].isin(\"ODD HOURS\", \"PHANTOM PREY\"))\n",
    "\n",
    "print(df_filtered.select(\"author\", \"title\", \"rank\", \"price\").show(10))\n",
    "\n",
    "# Is in \n",
    "df_filtered = dataframe.filter(dataframe[\"title\"].isin(\"ODD HOURS\", \"PHANTOM PREY\"))\n",
    "\n",
    "print(df_filtered.select(\"author\", \"title\", \"rank\", \"price\").show(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|               title|title LIKE %THE%|\n",
      "+--------------------+----------------+\n",
      "|           ODD HOURS|           false|\n",
      "|            THE HOST|            true|\n",
      "|LOVE THE ONE YOU'...|            true|\n",
      "|           THE FRONT|            true|\n",
      "|               SNUFF|           false|\n",
      "|SUNDAYS AT TIFFANY’S|           false|\n",
      "|        PHANTOM PREY|           false|\n",
      "|          SWINE NOT?|           false|\n",
      "|     CARELESS IN RED|           false|\n",
      "|     THE WHOLE TRUTH|            true|\n",
      "|          INVINCIBLE|           false|\n",
      "|BRIGHT SHINY MORNING|           false|\n",
      "|THE ART OF RACING...|            true|\n",
      "|       TWENTY WISHES|           false|\n",
      "|      THE STEEL WAVE|            true|\n",
      "| EXECUTIVE PRIVILEGE|           false|\n",
      "|  UNACCUSTOMED EARTH|           false|\n",
      "|          NETHERLAND|            true|\n",
      "|          THE APPEAL|            true|\n",
      "|INDIANA JONES AND...|            true|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Like\n",
    "dataframe.select(\"title\", dataframe.title.like(\"%THE%\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+\n",
      "|           author|               title|with_the|\n",
      "+-----------------+--------------------+--------+\n",
      "|    Dean R Koontz|           ODD HOURS|   false|\n",
      "|  Stephenie Meyer|            THE HOST|    true|\n",
      "|     Emily Giffin|LOVE THE ONE YOU'...|   false|\n",
      "|Patricia Cornwell|           THE FRONT|    true|\n",
      "|  Chuck Palahniuk|               SNUFF|   false|\n",
      "+-----------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# startswith y endswith \n",
    "\n",
    "dataframe.select(\"author\", \"title\", dataframe.title.startswith(\"THE\").alias(\"with_the\")).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|              author|               title|ends_with_nt|\n",
      "+--------------------+--------------------+------------+\n",
      "|       Dean R Koontz|           ODD HOURS|       false|\n",
      "|     Stephenie Meyer|            THE HOST|       false|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...|       false|\n",
      "|   Patricia Cornwell|           THE FRONT|        true|\n",
      "|     Chuck Palahniuk|               SNUFF|       false|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANY’S|       false|\n",
      "|       John Sandford|        PHANTOM PREY|       false|\n",
      "|       Jimmy Buffett|          SWINE NOT?|       false|\n",
      "|    Elizabeth George|     CARELESS IN RED|       false|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|       false|\n",
      "|        Troy Denning|          INVINCIBLE|       false|\n",
      "|          James Frey|BRIGHT SHINY MORNING|       false|\n",
      "|         Garth Stein|THE ART OF RACING...|       false|\n",
      "|     Debbie Macomber|       TWENTY WISHES|       false|\n",
      "|         Jeff Shaara|      THE STEEL WAVE|       false|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select(\"author\", \"title\", dataframe.title.endswith(\"NT\").alias(\"ends_with_nt\")).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Substr|\n",
      "+------+\n",
      "|Dean R|\n",
      "|Stephe|\n",
      "|Emily |\n",
      "|Patric|\n",
      "|Chuck |\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substring\n",
    "\n",
    "dataframe.select(dataframe.author.substr(1, 6).alias(\"Substr\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros 10195\n",
      "+--------------------+--------------------+\n",
      "|               title|              author|\n",
      "+--------------------+--------------------+\n",
      "|           ODD HOURS|       Dean R Koontz|\n",
      "|            THE HOST|     Stephenie Meyer|\n",
      "|LOVE THE ONE YOU'...|        Emily Giffin|\n",
      "|           THE FRONT|   Patricia Cornwell|\n",
      "|               SNUFF|     Chuck Palahniuk|\n",
      "|SUNDAYS AT TIFFANY’S|James Patterson a...|\n",
      "|        PHANTOM PREY|       John Sandford|\n",
      "|          SWINE NOT?|       Jimmy Buffett|\n",
      "|     CARELESS IN RED|    Elizabeth George|\n",
      "|     THE WHOLE TRUTH|      David Baldacci|\n",
      "|          INVINCIBLE|        Troy Denning|\n",
      "|BRIGHT SHINY MORNING|          James Frey|\n",
      "|THE ART OF RACING...|         Garth Stein|\n",
      "|       TWENTY WISHES|     Debbie Macomber|\n",
      "|      THE STEEL WAVE|         Jeff Shaara|\n",
      "| EXECUTIVE PRIVILEGE|    Phillip Margolin|\n",
      "|  UNACCUSTOMED EARTH|       Jhumpa Lahiri|\n",
      "|          NETHERLAND|      Joseph O'Neill|\n",
      "|          THE APPEAL|        John Grisham|\n",
      "|INDIANA JONES AND...|       James Rollins|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|                 _id|  amazon_product_url|              author| bestsellers_date|         description|   price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|              Bantam| [1]|           [0]|           ODD HOURS|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|       Little, Brown| [2]|           [1]|            THE HOST|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]|        St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|              Putnam| [4]|           [0]|           THE FRONT|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|           Doubleday| [5]|           [0]|               SNUFF|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|[[1211587200000]]|A woman finds an ...|[24.99,]|[[1212883200000]]|       Little, Brown| [6]|           [3]|SUNDAYS AT TIFFANY’S|          [4]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       John Sandford|[[1211587200000]]|The Minneapolis d...|[26.95,]|[[1212883200000]]|              Putnam| [7]|           [4]|        PHANTOM PREY|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|[[1211587200000]]|A Southern family...|[21.99,]|[[1212883200000]]|       Little, Brown| [8]|           [6]|          SWINE NOT?|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|[[1211587200000]]|In Cornwall, tryi...|[27.95,]|[[1212883200000]]|              Harper| [9]|           [8]|     CARELESS IN RED|          [3]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|[[1211587200000]]|An intelligence a...|[26.99,]|[[1212883200000]]|       Grand Central|[10]|           [7]|     THE WHOLE TRUTH|          [5]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|[[1211587200000]]|The New Jedi orde...|  [, 27]|[[1212883200000]]|  Del Rey/Ballantine|[11]|           [5]|          INVINCIBLE|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|          James Frey|[[1211587200000]]|A novel, set in L...|[26.95,]|[[1212883200000]]|              Harper|[12]|           [9]|BRIGHT SHINY MORNING|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|[[1211587200000]]|A Lab-terrier mix...|[23.95,]|[[1212883200000]]|              Harper|[13]|           [0]|THE ART OF RACING...|          [1]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|[[1211587200000]]|A widow who owns ...|[24.95,]|[[1212883200000]]|                Mira|[14]|          [10]|       TWENTY WISHES|          [4]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|[[1211587200000]]|A novel about the...|  [, 28]|[[1212883200000]]|          Ballantine|[15]|          [11]|      THE STEEL WAVE|          [2]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|[[1211587200000]]|                    |   [, 0]|[[1212883200000]]|HarperCollins Pub...|[16]|           [0]| EXECUTIVE PRIVILEGE|          [0]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|[[1211587200000]]|Stories of the an...|   [, 0]|[[1212883200000]]|               Knopf|[17]|           [0]|  UNACCUSTOMED EARTH|          [0]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|[[1211587200000]]|A Dutchman desert...|   [, 0]|[[1212883200000]]|Knopf Publishing ...|[18]|           [0]|          NETHERLAND|          [0]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|        John Grisham|[[1211587200000]]|Political and leg...|   [, 0]|[[1212883200000]]|Doubleday Publishing|[19]|           [0]|          THE APPEAL|          [0]|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|       James Rollins|[[1211587200000]]|                    |   [, 0]|[[1212883200000]]|Random House Publ...|[20]|           [0]|INDIANA JONES AND...|          [0]|\n",
      "+--------------------+--------------------+--------------------+-----------------+--------------------+--------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+-------------+-----------------+--------------------+------+-----------------+---------+----+--------------+---------+-------------+\n",
      "|                 _id|  amazon_product_url|       author| bestsellers_date|         description| price|   published_date|publisher|rank|rank_last_week|    title|weeks_on_list|\n",
      "+--------------------+--------------------+-------------+-----------------+--------------------+------+-----------------+---------+----+--------------+---------+-------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|[, 27]|[[1212883200000]]|   Bantam| [1]|           [0]|ODD HOURS|          [1]|\n",
      "+--------------------+--------------------+-------------+-----------------+--------------------+------+-----------------+---------+----+--------------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultas SQL\n",
    "\n",
    "#Total de registros\n",
    "print(\"Total de registros\", dataframe2.count())\n",
    "\n",
    "# Se debe primero registrar la tabla\n",
    "dataframe.registerTempTable(\"books\")\n",
    "\n",
    "spark.sql(\"SELECT title, author FROM books\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM books\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM books where title = 'ODD HOURS'\").show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: struct<$oid:string>, amazon_product_url: string, author: string, bestsellers_date: struct<$date:struct<$numberLong:string>>, description: string, price: struct<$numberDouble:string,$numberInt:string>, published_date: struct<$date:struct<$numberLong:string>>, publisher: string, rank: struct<$numberInt:string>, rank_last_week: struct<$numberInt:string>, title: string, weeks_on_list: struct<$numberInt:string>, new_column: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|          new_column|               title|  amazon_product_url|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|           ODD HOURS|           ODD HOURS|http://www.amazon...|\n",
      "|            THE HOST|            THE HOST|http://www.amazon...|\n",
      "|LOVE THE ONE YOU'...|LOVE THE ONE YOU'...|http://www.amazon...|\n",
      "|           THE FRONT|           THE FRONT|http://www.amazon...|\n",
      "|               SNUFF|               SNUFF|http://www.amazon...|\n",
      "|SUNDAYS AT TIFFANY’S|SUNDAYS AT TIFFANY’S|http://www.amazon...|\n",
      "|        PHANTOM PREY|        PHANTOM PREY|http://www.amazon...|\n",
      "|          SWINE NOT?|          SWINE NOT?|http://www.amazon...|\n",
      "|     CARELESS IN RED|     CARELESS IN RED|http://www.amazon...|\n",
      "|     THE WHOLE TRUTH|     THE WHOLE TRUTH|http://www.amazon...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Agregando, borrando y renombrando columnas\n",
    "\n",
    "# Agregar columna\n",
    "dataframe2 = dataframe.withColumn('new_column', dataframe.title)\n",
    "display(dataframe2)\n",
    "\n",
    "print(dataframe2.select('new_column',\"title\", \"amazon_product_url\").show(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renombrar columna\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 URL|           author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|           ODD HOURS|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|            THE HOST|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|LOVE THE ONE YOU'...|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|       Putnam| [4]|           [0]|           THE FRONT|          [1]|           THE FRONT|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|    Doubleday| [5]|           [0]|               SNUFF|          [1]|               SNUFF|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renombrar columna\n",
    "print(\"Renombrar columna\")\n",
    "dataframe3 = dataframe2.withColumnRenamed('amazon_product_url', 'URL')\n",
    "dataframe3.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|           author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|           ODD HOURS|\n",
      "|[5b4aa4ead3089013...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|            THE HOST|\n",
      "|[5b4aa4ead3089013...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|LOVE THE ONE YOU'...|\n",
      "|[5b4aa4ead3089013...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|       Putnam| [4]|           [0]|           THE FRONT|          [1]|           THE FRONT|\n",
      "|[5b4aa4ead3089013...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|    Doubleday| [5]|           [0]|               SNUFF|          [1]|               SNUFF|\n",
      "+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Borrar una columna\n",
    "dataframe_remove = dataframe3.drop(\"URL\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar datos a disco en un formato particular\n",
    "dataframe.select(\"author\", \"title\") \\\n",
    ".write \\\n",
    ".save(\"Authors_Titles2.json\",format=\"json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.select(\"author\", \"title\") \\\n",
    ".write \\\n",
    ".save(\"Authors_Titles3.csv\",format=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Salvar datos a PostgreSQL\n",
    "def write_spark_df_to_db(spark_df, table_name):\n",
    "    \"\"\"\n",
    "    This function writes Spark dataframe to DB\n",
    "    \"\"\"\n",
    "    spark_df \\\n",
    "        .write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .mode('overwrite') \\\n",
    "        .option(\"url\", POSTGRESQL_URL) \\\n",
    "        .option(\"user\", POSTGRESQL_USER) \\\n",
    "        .option(\"password\", POSTGRESQL_PASSWORD) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .save()\n",
    "    \n",
    "write_spark_df_to_db(dataframe.select(\"author\", \"title\"), \"prueba\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "[1] Spark SQL, DataFrames and Datasets Guide. Recuperado de https://spark.apache.org/docs/2.3.0/sql-programming-guide.html\n",
    "\n",
    "[2] Esquivel, J. (2019). Big Data. Ciencias de los datos. ITCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AlgebraLinealEjemplos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
